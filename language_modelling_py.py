# -*- coding: utf-8 -*-
"""Language_Modelling.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qimxzPF2VxHjtv93hyqoeDbXaxGK5pqL
"""

import os, random, math
from google.colab import drive

drive.mount('/content/drive')

TRAINING_DIR="drive/My Drive/ML/AdvNLP/sentence-completion/Holmes_Training_Data"

TRAINING_DIR="drive/My Drive/ML/AdvNLP/sentence-completion/Holmes_Training_Data"

def get_training_testing(training_dir=TRAINING_DIR,split=0.5):
  filenames=os.listdir(training_dir) 
  n=len(filenames)
  print("There are {} files in the training directory ".format(n,training_dir)) 
  random.seed(53) 
  random.shuffle(filenames)
  index=int(n*split) 
  return(filenames[:index],filenames[index:])

trainingfiles,heldoutfiles=get_training_testing()

from nltk import word_tokenize as tokenize 
import operator
import random 
from operator import itemgetter
import math 
#Creates Unigram Model 

class language_model():
  def __init__(self,trainingdir=TRAINING_DIR,files=[]):
    self.training_dir=trainingdir
    self.files=files
    self.train()
  def train(self): 
    self.unigram= {} 
    #adds bigram
    self.bigram = {}
    self.trigram = {}
    self._processfiles()
    #accounts for unknown
    self.account_for_unknown()
    self._convert_to_probs()

  def _processline(self,line): 
    tokens=["_START"]+tokenize(line)+["_END"] 
    for count,token in enumerate(tokens):   #added enumerate
      
      self.unigram[token]=self.unigram.get(token,0)+1
      #for bigrams
      current = self.bigram.get(token, {})
      if count < len(tokens) - 1: 
        partner = tokens[count+1] #accounts for _END on each line 
      else: 
        pass
      current[partner] = current.get(partner, 0) + 1
      self.bigram[token] = current

      #for trigrams
      current_3 = self.trigram.get(token, {})
      if count < len(tokens) - 2: 
        partner_3 =" ".join(tokens[count+1: count+3])#accounts for _END on each line 
        current_3[partner_3] = current_3.get(partner_3, 0) + 1
        self.trigram[token] = current_3
      else: 
        pass

  def _processfiles(self):
     for afile in self.files:
        print("Processing".format(afile))
        try: 
          with open(os.path.join(self.training_dir,afile)) as instream: 
            for line in instream: 
              line=line.rstrip() 
              if len(line)>0: 
                self._processline(line)
        except UnicodeDecodeError: 
          print("UnicodeDecodeError processing  {}: ignoring file".format(afile))

  def _convert_to_probs(self):
    self.unigram= {k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}
    self.bigram= {key: {word:count/sum(partner.values()) for word,count in partner.items()} for key,partner in self.bigram.items()}

          
  
  def get_prob(self,token,method="unigram"): 
    """Token is alist"""

    if method.lower() == "unigram":
      try:
        return self.unigram.get(token)
      except KeyError:
        return self.unigram.get('_UNK') 

    #adds bigram probability
    elif method.lower == "bigram" :
      try:
        current,next = token
        return self.bigram.get(current)[next]
      except KeyError:
        try:
          return self.bigram.get('_UNK')[next]
        except KeyError:
           return self.bigram.get('_UNK')['_UNK']
          
    #checks trigram probability
    #elif method.lower() == "trigram":
    else: 
      print("Not implemented: {}".format(method)) 
      return 0

  def display_bigram(self):
    return self.bigram
  def display_unigram(self):
    return self.unigram
  def display_trigram(self):
    return self.trigram

  def gen_sentence(self,start_token,num_of_words = 2,k = 5):
    """Generates sentences with from start token using bigrams, selects from the top k possibilities"""

    sentence = start_token
    last_token = start_token
    while num_of_words > 1:

      dict_possible_next_word = self.bigram.get(last_token)
      #random_next_word = tuple(dict_random_next_word)
      possible_next_word = sorted(dict_possible_next_word.items(), key = itemgetter(1), reverse = True)
      random_next_word = [key for key,_ in possible_next_word]
        
      if len(random_next_word) >= k: k = k
      else :k = len(random_next_word)

      n = random.randint(0, k)
      next_word = ' ' + random_next_word[n]
      sentence =  sentence + next_word
      
      num_of_words -= 1
      last_token = next_word

    return sentence

  def conv_to_log_probs(self):
    self.unigram= {k:max(0,math.log(v/sum(self.unigram.values()))) for (k,v) in self.unigram.items()}
    self.bigram= {key: {word:max(0, math.log(count/sum(partner.values()))) for word,count in partner.items()} for key,partner in self.bigram.items()}
    self.n = len(self.unigram)
    return self.unigram, self.bigram, self.n

  def perplexity(self,corpus, method = "unigram"):
    """Computes the perplexity of a corpus, high perplexity means it's not possible"""
    tokens = tokenize(corpus)
    self.lp = {}
    for token in tokens:
      self.lp[token] = self.unigram(token, method)
    
    return math.exp(-sum(self.lp.values())/self.n)

  def account_for_unknown(self, threshold = 1):
    """Function  accounts for unknown words in test_files by converting word with count lesser than threshold in training data into _UNK and using p(_UNK) for unknown words"""

    for (k,value) in list(self.unigram.items()):
      if value <= threshold:
        self.unigram['_UNK'] = self.unigram.get('_UNK', 0) + value
        del self.unigram[k]
      else:
        pass

    self.bigram['_UNK'] = {}

    for key,adict in list(self.bigram.items()):
      if key in self.unigram.keys():
        for next_word,value in list(adict.items()):
          if value <= threshold:
            adict['_UNK'] =adict.get('_UNK', 0 ) + value
            del adict[next_word]
          else: pass
      else: 
        current = self.bigram.get('_UNK', {}) 
        for next_word,count in adict.items():
          score = current.get(next_word, 0) + count
          self.bigram['_UNK'][next_word] = score  
        del self.bigram[key]
    


  
#Add bigram function 
#Compute bigram probabilities
#Generate plausible sounding sentences by using the current token as start and randomly choose one of the top k most probable words

#Write a function that calculates the perplexity of a model and the language data using the unigram model and bigram model
  #def log_probability(corpus)
  
  #def perplex_ity(self,corpus):
    #np.exp(-self.log_probability()/len(self.unigram())

#Dealing with unseen data, add-one smoothing? #Add-one smoothing not viable because it attributes a lot of probability mass to unseen events and is computationally inefficient
#Replacing low frequency unigrams in the training data with the _UNK token (if count < certain threshold  before converting into probabilities)
#Adjust in the bigram distribution 
#When testing, if the probability of the word is not known, use the probability of _UNK instead



#Discounting for unseen combinations in bigrams

#Investigate how perplexity is affected by the amount of training data, amount of testing data, threshold frequency for unknown words

#Create trigrams

import nltk 
nltk.download('punkt')

#Testing with just five files for functionality to reduce load

Max_files = 5
mylm = language_model(files = trainingfiles[:Max_files])
test_data = heldoutfiles[Max_files:10]
mylm.train()

#mylm.display_unigram()
#mylm.display_bigram()
mylm.display_trigram()
#mylm.get_prob('for', method = "Unigram")

mylm.gen_sentence('Father', num_of_words = 4)

